{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's Cooking - 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aka first attempt at the Kaggle 'What's Cooking?' competition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON\n",
    "First I need to learn to read in the data, which is in JSON format. What does that mean? \n",
    "\n",
    "JSON means JavaScript Object Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON is an open-standard format that uses human-readable text to transmit data objects consisting of attribute-value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-177-02467aedea86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'train.json'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Elizabeth/anaconda/lib/python2.7/json/__init__.pyc\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Elizabeth/anaconda/lib/python2.7/json/decoder.pyc\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \"\"\"\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or buffer"
     ]
    }
   ],
   "source": [
    "data = json.loads({'train.json'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package json:\n",
      "\n",
      "NAME\n",
      "    json\n",
      "\n",
      "FILE\n",
      "    /Users/Elizabeth/anaconda/lib/python2.7/json/__init__.py\n",
      "\n",
      "MODULE DOCS\n",
      "    http://docs.python.org/library/json\n",
      "\n",
      "DESCRIPTION\n",
      "    JSON (JavaScript Object Notation) <http://json.org> is a subset of\n",
      "    JavaScript syntax (ECMA-262 3rd edition) used as a lightweight data\n",
      "    interchange format.\n",
      "    \n",
      "    :mod:`json` exposes an API familiar to users of the standard library\n",
      "    :mod:`marshal` and :mod:`pickle` modules. It is the externally maintained\n",
      "    version of the :mod:`json` library contained in Python 2.6, but maintains\n",
      "    compatibility with Python 2.4 and Python 2.5 and (currently) has\n",
      "    significant performance advantages, even without using the optional C\n",
      "    extension for speedups.\n",
      "    \n",
      "    Encoding basic Python object hierarchies::\n",
      "    \n",
      "        >>> import json\n",
      "        >>> json.dumps(['foo', {'bar': ('baz', None, 1.0, 2)}])\n",
      "        '[\"foo\", {\"bar\": [\"baz\", null, 1.0, 2]}]'\n",
      "        >>> print json.dumps(\"\\\"foo\\bar\")\n",
      "        \"\\\"foo\\bar\"\n",
      "        >>> print json.dumps(u'\\u1234')\n",
      "        \"\\u1234\"\n",
      "        >>> print json.dumps('\\\\')\n",
      "        \"\\\\\"\n",
      "        >>> print json.dumps({\"c\": 0, \"b\": 0, \"a\": 0}, sort_keys=True)\n",
      "        {\"a\": 0, \"b\": 0, \"c\": 0}\n",
      "        >>> from StringIO import StringIO\n",
      "        >>> io = StringIO()\n",
      "        >>> json.dump(['streaming API'], io)\n",
      "        >>> io.getvalue()\n",
      "        '[\"streaming API\"]'\n",
      "    \n",
      "    Compact encoding::\n",
      "    \n",
      "        >>> import json\n",
      "        >>> json.dumps([1,2,3,{'4': 5, '6': 7}], sort_keys=True, separators=(',',':'))\n",
      "        '[1,2,3,{\"4\":5,\"6\":7}]'\n",
      "    \n",
      "    Pretty printing::\n",
      "    \n",
      "        >>> import json\n",
      "        >>> print json.dumps({'4': 5, '6': 7}, sort_keys=True,\n",
      "        ...                  indent=4, separators=(',', ': '))\n",
      "        {\n",
      "            \"4\": 5,\n",
      "            \"6\": 7\n",
      "        }\n",
      "    \n",
      "    Decoding JSON::\n",
      "    \n",
      "        >>> import json\n",
      "        >>> obj = [u'foo', {u'bar': [u'baz', None, 1.0, 2]}]\n",
      "        >>> json.loads('[\"foo\", {\"bar\":[\"baz\", null, 1.0, 2]}]') == obj\n",
      "        True\n",
      "        >>> json.loads('\"\\\\\"foo\\\\bar\"') == u'\"foo\\x08ar'\n",
      "        True\n",
      "        >>> from StringIO import StringIO\n",
      "        >>> io = StringIO('[\"streaming API\"]')\n",
      "        >>> json.load(io)[0] == 'streaming API'\n",
      "        True\n",
      "    \n",
      "    Specializing JSON object decoding::\n",
      "    \n",
      "        >>> import json\n",
      "        >>> def as_complex(dct):\n",
      "        ...     if '__complex__' in dct:\n",
      "        ...         return complex(dct['real'], dct['imag'])\n",
      "        ...     return dct\n",
      "        ...\n",
      "        >>> json.loads('{\"__complex__\": true, \"real\": 1, \"imag\": 2}',\n",
      "        ...     object_hook=as_complex)\n",
      "        (1+2j)\n",
      "        >>> from decimal import Decimal\n",
      "        >>> json.loads('1.1', parse_float=Decimal) == Decimal('1.1')\n",
      "        True\n",
      "    \n",
      "    Specializing JSON object encoding::\n",
      "    \n",
      "        >>> import json\n",
      "        >>> def encode_complex(obj):\n",
      "        ...     if isinstance(obj, complex):\n",
      "        ...         return [obj.real, obj.imag]\n",
      "        ...     raise TypeError(repr(o) + \" is not JSON serializable\")\n",
      "        ...\n",
      "        >>> json.dumps(2 + 1j, default=encode_complex)\n",
      "        '[2.0, 1.0]'\n",
      "        >>> json.JSONEncoder(default=encode_complex).encode(2 + 1j)\n",
      "        '[2.0, 1.0]'\n",
      "        >>> ''.join(json.JSONEncoder(default=encode_complex).iterencode(2 + 1j))\n",
      "        '[2.0, 1.0]'\n",
      "    \n",
      "    \n",
      "    Using json.tool from the shell to validate and pretty-print::\n",
      "    \n",
      "        $ echo '{\"json\":\"obj\"}' | python -m json.tool\n",
      "        {\n",
      "            \"json\": \"obj\"\n",
      "        }\n",
      "        $ echo '{ 1.2:3.4}' | python -m json.tool\n",
      "        Expecting property name enclosed in double quotes: line 1 column 3 (char 2)\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    decoder\n",
      "    encoder\n",
      "    scanner\n",
      "    tests (package)\n",
      "    tool\n",
      "\n",
      "CLASSES\n",
      "    __builtin__.object\n",
      "        json.decoder.JSONDecoder\n",
      "        json.encoder.JSONEncoder\n",
      "    \n",
      "    class JSONDecoder(__builtin__.object)\n",
      "     |  Simple JSON <http://json.org> decoder\n",
      "     |  \n",
      "     |  Performs the following translations in decoding by default:\n",
      "     |  \n",
      "     |  +---------------+-------------------+\n",
      "     |  | JSON          | Python            |\n",
      "     |  +===============+===================+\n",
      "     |  | object        | dict              |\n",
      "     |  +---------------+-------------------+\n",
      "     |  | array         | list              |\n",
      "     |  +---------------+-------------------+\n",
      "     |  | string        | unicode           |\n",
      "     |  +---------------+-------------------+\n",
      "     |  | number (int)  | int, long         |\n",
      "     |  +---------------+-------------------+\n",
      "     |  | number (real) | float             |\n",
      "     |  +---------------+-------------------+\n",
      "     |  | true          | True              |\n",
      "     |  +---------------+-------------------+\n",
      "     |  | false         | False             |\n",
      "     |  +---------------+-------------------+\n",
      "     |  | null          | None              |\n",
      "     |  +---------------+-------------------+\n",
      "     |  \n",
      "     |  It also understands ``NaN``, ``Infinity``, and ``-Infinity`` as\n",
      "     |  their corresponding ``float`` values, which is outside the JSON spec.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, encoding=None, object_hook=None, parse_float=None, parse_int=None, parse_constant=None, strict=True, object_pairs_hook=None)\n",
      "     |      ``encoding`` determines the encoding used to interpret any ``str``\n",
      "     |      objects decoded by this instance (utf-8 by default).  It has no\n",
      "     |      effect when decoding ``unicode`` objects.\n",
      "     |      \n",
      "     |      Note that currently only encodings that are a superset of ASCII work,\n",
      "     |      strings of other encodings should be passed in as ``unicode``.\n",
      "     |      \n",
      "     |      ``object_hook``, if specified, will be called with the result\n",
      "     |      of every JSON object decoded and its return value will be used in\n",
      "     |      place of the given ``dict``.  This can be used to provide custom\n",
      "     |      deserializations (e.g. to support JSON-RPC class hinting).\n",
      "     |      \n",
      "     |      ``object_pairs_hook``, if specified will be called with the result of\n",
      "     |      every JSON object decoded with an ordered list of pairs.  The return\n",
      "     |      value of ``object_pairs_hook`` will be used instead of the ``dict``.\n",
      "     |      This feature can be used to implement custom decoders that rely on the\n",
      "     |      order that the key and value pairs are decoded (for example,\n",
      "     |      collections.OrderedDict will remember the order of insertion). If\n",
      "     |      ``object_hook`` is also defined, the ``object_pairs_hook`` takes\n",
      "     |      priority.\n",
      "     |      \n",
      "     |      ``parse_float``, if specified, will be called with the string\n",
      "     |      of every JSON float to be decoded. By default this is equivalent to\n",
      "     |      float(num_str). This can be used to use another datatype or parser\n",
      "     |      for JSON floats (e.g. decimal.Decimal).\n",
      "     |      \n",
      "     |      ``parse_int``, if specified, will be called with the string\n",
      "     |      of every JSON int to be decoded. By default this is equivalent to\n",
      "     |      int(num_str). This can be used to use another datatype or parser\n",
      "     |      for JSON integers (e.g. float).\n",
      "     |      \n",
      "     |      ``parse_constant``, if specified, will be called with one of the\n",
      "     |      following strings: -Infinity, Infinity, NaN.\n",
      "     |      This can be used to raise an exception if invalid JSON numbers\n",
      "     |      are encountered.\n",
      "     |      \n",
      "     |      If ``strict`` is false (true is the default), then control\n",
      "     |      characters will be allowed inside strings.  Control characters in\n",
      "     |      this context are those with character codes in the 0-31 range,\n",
      "     |      including ``'\\t'`` (tab), ``'\\n'``, ``'\\r'`` and ``'\\0'``.\n",
      "     |  \n",
      "     |  decode(self, s, _w=<built-in method match of _sre.SRE_Pattern object>)\n",
      "     |      Return the Python representation of ``s`` (a ``str`` or ``unicode``\n",
      "     |      instance containing a JSON document)\n",
      "     |  \n",
      "     |  raw_decode(self, s, idx=0)\n",
      "     |      Decode a JSON document from ``s`` (a ``str`` or ``unicode``\n",
      "     |      beginning with a JSON document) and return a 2-tuple of the Python\n",
      "     |      representation and the index in ``s`` where the document ended.\n",
      "     |      \n",
      "     |      This can be used to decode a JSON document from a string that may\n",
      "     |      have extraneous data at the end.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class JSONEncoder(__builtin__.object)\n",
      "     |  Extensible JSON <http://json.org> encoder for Python data structures.\n",
      "     |  \n",
      "     |  Supports the following objects and types by default:\n",
      "     |  \n",
      "     |  +-------------------+---------------+\n",
      "     |  | Python            | JSON          |\n",
      "     |  +===================+===============+\n",
      "     |  | dict              | object        |\n",
      "     |  +-------------------+---------------+\n",
      "     |  | list, tuple       | array         |\n",
      "     |  +-------------------+---------------+\n",
      "     |  | str, unicode      | string        |\n",
      "     |  +-------------------+---------------+\n",
      "     |  | int, long, float  | number        |\n",
      "     |  +-------------------+---------------+\n",
      "     |  | True              | true          |\n",
      "     |  +-------------------+---------------+\n",
      "     |  | False             | false         |\n",
      "     |  +-------------------+---------------+\n",
      "     |  | None              | null          |\n",
      "     |  +-------------------+---------------+\n",
      "     |  \n",
      "     |  To extend this to recognize other objects, subclass and implement a\n",
      "     |  ``.default()`` method with another method that returns a serializable\n",
      "     |  object for ``o`` if possible, otherwise it should call the superclass\n",
      "     |  implementation (to raise ``TypeError``).\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, skipkeys=False, ensure_ascii=True, check_circular=True, allow_nan=True, sort_keys=False, indent=None, separators=None, encoding='utf-8', default=None)\n",
      "     |      Constructor for JSONEncoder, with sensible defaults.\n",
      "     |      \n",
      "     |      If skipkeys is false, then it is a TypeError to attempt\n",
      "     |      encoding of keys that are not str, int, long, float or None.  If\n",
      "     |      skipkeys is True, such items are simply skipped.\n",
      "     |      \n",
      "     |      If *ensure_ascii* is true (the default), all non-ASCII\n",
      "     |      characters in the output are escaped with \\uXXXX sequences,\n",
      "     |      and the results are str instances consisting of ASCII\n",
      "     |      characters only.  If ensure_ascii is False, a result may be a\n",
      "     |      unicode instance.  This usually happens if the input contains\n",
      "     |      unicode strings or the *encoding* parameter is used.\n",
      "     |      \n",
      "     |      If check_circular is true, then lists, dicts, and custom encoded\n",
      "     |      objects will be checked for circular references during encoding to\n",
      "     |      prevent an infinite recursion (which would cause an OverflowError).\n",
      "     |      Otherwise, no such check takes place.\n",
      "     |      \n",
      "     |      If allow_nan is true, then NaN, Infinity, and -Infinity will be\n",
      "     |      encoded as such.  This behavior is not JSON specification compliant,\n",
      "     |      but is consistent with most JavaScript based encoders and decoders.\n",
      "     |      Otherwise, it will be a ValueError to encode such floats.\n",
      "     |      \n",
      "     |      If sort_keys is true, then the output of dictionaries will be\n",
      "     |      sorted by key; this is useful for regression tests to ensure\n",
      "     |      that JSON serializations can be compared on a day-to-day basis.\n",
      "     |      \n",
      "     |      If indent is a non-negative integer, then JSON array\n",
      "     |      elements and object members will be pretty-printed with that\n",
      "     |      indent level.  An indent level of 0 will only insert newlines.\n",
      "     |      None is the most compact representation.  Since the default\n",
      "     |      item separator is ', ',  the output might include trailing\n",
      "     |      whitespace when indent is specified.  You can use\n",
      "     |      separators=(',', ': ') to avoid this.\n",
      "     |      \n",
      "     |      If specified, separators should be a (item_separator, key_separator)\n",
      "     |      tuple.  The default is (', ', ': ').  To get the most compact JSON\n",
      "     |      representation you should specify (',', ':') to eliminate whitespace.\n",
      "     |      \n",
      "     |      If specified, default is a function that gets called for objects\n",
      "     |      that can't otherwise be serialized.  It should return a JSON encodable\n",
      "     |      version of the object or raise a ``TypeError``.\n",
      "     |      \n",
      "     |      If encoding is not None, then all input strings will be\n",
      "     |      transformed into unicode using that encoding prior to JSON-encoding.\n",
      "     |      The default is UTF-8.\n",
      "     |  \n",
      "     |  default(self, o)\n",
      "     |      Implement this method in a subclass such that it returns\n",
      "     |      a serializable object for ``o``, or calls the base implementation\n",
      "     |      (to raise a ``TypeError``).\n",
      "     |      \n",
      "     |      For example, to support arbitrary iterators, you could\n",
      "     |      implement default like this::\n",
      "     |      \n",
      "     |          def default(self, o):\n",
      "     |              try:\n",
      "     |                  iterable = iter(o)\n",
      "     |              except TypeError:\n",
      "     |                  pass\n",
      "     |              else:\n",
      "     |                  return list(iterable)\n",
      "     |              # Let the base class default method raise the TypeError\n",
      "     |              return JSONEncoder.default(self, o)\n",
      "     |  \n",
      "     |  encode(self, o)\n",
      "     |      Return a JSON string representation of a Python data structure.\n",
      "     |      \n",
      "     |      >>> JSONEncoder().encode({\"foo\": [\"bar\", \"baz\"]})\n",
      "     |      '{\"foo\": [\"bar\", \"baz\"]}'\n",
      "     |  \n",
      "     |  iterencode(self, o, _one_shot=False)\n",
      "     |      Encode the given object and yield each string\n",
      "     |      representation as available.\n",
      "     |      \n",
      "     |      For example::\n",
      "     |      \n",
      "     |          for chunk in JSONEncoder().iterencode(bigobject):\n",
      "     |              mysocket.write(chunk)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  item_separator = ', '\n",
      "     |  \n",
      "     |  key_separator = ': '\n",
      "\n",
      "FUNCTIONS\n",
      "    dump(obj, fp, skipkeys=False, ensure_ascii=True, check_circular=True, allow_nan=True, cls=None, indent=None, separators=None, encoding='utf-8', default=None, sort_keys=False, **kw)\n",
      "        Serialize ``obj`` as a JSON formatted stream to ``fp`` (a\n",
      "        ``.write()``-supporting file-like object).\n",
      "        \n",
      "        If ``skipkeys`` is true then ``dict`` keys that are not basic types\n",
      "        (``str``, ``unicode``, ``int``, ``long``, ``float``, ``bool``, ``None``)\n",
      "        will be skipped instead of raising a ``TypeError``.\n",
      "        \n",
      "        If ``ensure_ascii`` is true (the default), all non-ASCII characters in the\n",
      "        output are escaped with ``\\uXXXX`` sequences, and the result is a ``str``\n",
      "        instance consisting of ASCII characters only.  If ``ensure_ascii`` is\n",
      "        ``False``, some chunks written to ``fp`` may be ``unicode`` instances.\n",
      "        This usually happens because the input contains unicode strings or the\n",
      "        ``encoding`` parameter is used. Unless ``fp.write()`` explicitly\n",
      "        understands ``unicode`` (as in ``codecs.getwriter``) this is likely to\n",
      "        cause an error.\n",
      "        \n",
      "        If ``check_circular`` is false, then the circular reference check\n",
      "        for container types will be skipped and a circular reference will\n",
      "        result in an ``OverflowError`` (or worse).\n",
      "        \n",
      "        If ``allow_nan`` is false, then it will be a ``ValueError`` to\n",
      "        serialize out of range ``float`` values (``nan``, ``inf``, ``-inf``)\n",
      "        in strict compliance of the JSON specification, instead of using the\n",
      "        JavaScript equivalents (``NaN``, ``Infinity``, ``-Infinity``).\n",
      "        \n",
      "        If ``indent`` is a non-negative integer, then JSON array elements and\n",
      "        object members will be pretty-printed with that indent level. An indent\n",
      "        level of 0 will only insert newlines. ``None`` is the most compact\n",
      "        representation.  Since the default item separator is ``', '``,  the\n",
      "        output might include trailing whitespace when ``indent`` is specified.\n",
      "        You can use ``separators=(',', ': ')`` to avoid this.\n",
      "        \n",
      "        If ``separators`` is an ``(item_separator, dict_separator)`` tuple\n",
      "        then it will be used instead of the default ``(', ', ': ')`` separators.\n",
      "        ``(',', ':')`` is the most compact JSON representation.\n",
      "        \n",
      "        ``encoding`` is the character encoding for str instances, default is UTF-8.\n",
      "        \n",
      "        ``default(obj)`` is a function that should return a serializable version\n",
      "        of obj or raise TypeError. The default simply raises TypeError.\n",
      "        \n",
      "        If *sort_keys* is ``True`` (default: ``False``), then the output of\n",
      "        dictionaries will be sorted by key.\n",
      "        \n",
      "        To use a custom ``JSONEncoder`` subclass (e.g. one that overrides the\n",
      "        ``.default()`` method to serialize additional types), specify it with\n",
      "        the ``cls`` kwarg; otherwise ``JSONEncoder`` is used.\n",
      "    \n",
      "    dumps(obj, skipkeys=False, ensure_ascii=True, check_circular=True, allow_nan=True, cls=None, indent=None, separators=None, encoding='utf-8', default=None, sort_keys=False, **kw)\n",
      "        Serialize ``obj`` to a JSON formatted ``str``.\n",
      "        \n",
      "        If ``skipkeys`` is true then ``dict`` keys that are not basic types\n",
      "        (``str``, ``unicode``, ``int``, ``long``, ``float``, ``bool``, ``None``)\n",
      "        will be skipped instead of raising a ``TypeError``.\n",
      "        \n",
      "        \n",
      "        If ``ensure_ascii`` is false, all non-ASCII characters are not escaped, and\n",
      "        the return value may be a ``unicode`` instance. See ``dump`` for details.\n",
      "        \n",
      "        If ``check_circular`` is false, then the circular reference check\n",
      "        for container types will be skipped and a circular reference will\n",
      "        result in an ``OverflowError`` (or worse).\n",
      "        \n",
      "        If ``allow_nan`` is false, then it will be a ``ValueError`` to\n",
      "        serialize out of range ``float`` values (``nan``, ``inf``, ``-inf``) in\n",
      "        strict compliance of the JSON specification, instead of using the\n",
      "        JavaScript equivalents (``NaN``, ``Infinity``, ``-Infinity``).\n",
      "        \n",
      "        If ``indent`` is a non-negative integer, then JSON array elements and\n",
      "        object members will be pretty-printed with that indent level. An indent\n",
      "        level of 0 will only insert newlines. ``None`` is the most compact\n",
      "        representation.  Since the default item separator is ``', '``,  the\n",
      "        output might include trailing whitespace when ``indent`` is specified.\n",
      "        You can use ``separators=(',', ': ')`` to avoid this.\n",
      "        \n",
      "        If ``separators`` is an ``(item_separator, dict_separator)`` tuple\n",
      "        then it will be used instead of the default ``(', ', ': ')`` separators.\n",
      "        ``(',', ':')`` is the most compact JSON representation.\n",
      "        \n",
      "        ``encoding`` is the character encoding for str instances, default is UTF-8.\n",
      "        \n",
      "        ``default(obj)`` is a function that should return a serializable version\n",
      "        of obj or raise TypeError. The default simply raises TypeError.\n",
      "        \n",
      "        If *sort_keys* is ``True`` (default: ``False``), then the output of\n",
      "        dictionaries will be sorted by key.\n",
      "        \n",
      "        To use a custom ``JSONEncoder`` subclass (e.g. one that overrides the\n",
      "        ``.default()`` method to serialize additional types), specify it with\n",
      "        the ``cls`` kwarg; otherwise ``JSONEncoder`` is used.\n",
      "    \n",
      "    load(fp, encoding=None, cls=None, object_hook=None, parse_float=None, parse_int=None, parse_constant=None, object_pairs_hook=None, **kw)\n",
      "        Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\n",
      "        a JSON document) to a Python object.\n",
      "        \n",
      "        If the contents of ``fp`` is encoded with an ASCII based encoding other\n",
      "        than utf-8 (e.g. latin-1), then an appropriate ``encoding`` name must\n",
      "        be specified. Encodings that are not ASCII based (such as UCS-2) are\n",
      "        not allowed, and should be wrapped with\n",
      "        ``codecs.getreader(fp)(encoding)``, or simply decoded to a ``unicode``\n",
      "        object and passed to ``loads()``\n",
      "        \n",
      "        ``object_hook`` is an optional function that will be called with the\n",
      "        result of any object literal decode (a ``dict``). The return value of\n",
      "        ``object_hook`` will be used instead of the ``dict``. This feature\n",
      "        can be used to implement custom decoders (e.g. JSON-RPC class hinting).\n",
      "        \n",
      "        ``object_pairs_hook`` is an optional function that will be called with the\n",
      "        result of any object literal decoded with an ordered list of pairs.  The\n",
      "        return value of ``object_pairs_hook`` will be used instead of the ``dict``.\n",
      "        This feature can be used to implement custom decoders that rely on the\n",
      "        order that the key and value pairs are decoded (for example,\n",
      "        collections.OrderedDict will remember the order of insertion). If\n",
      "        ``object_hook`` is also defined, the ``object_pairs_hook`` takes priority.\n",
      "        \n",
      "        To use a custom ``JSONDecoder`` subclass, specify it with the ``cls``\n",
      "        kwarg; otherwise ``JSONDecoder`` is used.\n",
      "    \n",
      "    loads(s, encoding=None, cls=None, object_hook=None, parse_float=None, parse_int=None, parse_constant=None, object_pairs_hook=None, **kw)\n",
      "        Deserialize ``s`` (a ``str`` or ``unicode`` instance containing a JSON\n",
      "        document) to a Python object.\n",
      "        \n",
      "        If ``s`` is a ``str`` instance and is encoded with an ASCII based encoding\n",
      "        other than utf-8 (e.g. latin-1) then an appropriate ``encoding`` name\n",
      "        must be specified. Encodings that are not ASCII based (such as UCS-2)\n",
      "        are not allowed and should be decoded to ``unicode`` first.\n",
      "        \n",
      "        ``object_hook`` is an optional function that will be called with the\n",
      "        result of any object literal decode (a ``dict``). The return value of\n",
      "        ``object_hook`` will be used instead of the ``dict``. This feature\n",
      "        can be used to implement custom decoders (e.g. JSON-RPC class hinting).\n",
      "        \n",
      "        ``object_pairs_hook`` is an optional function that will be called with the\n",
      "        result of any object literal decoded with an ordered list of pairs.  The\n",
      "        return value of ``object_pairs_hook`` will be used instead of the ``dict``.\n",
      "        This feature can be used to implement custom decoders that rely on the\n",
      "        order that the key and value pairs are decoded (for example,\n",
      "        collections.OrderedDict will remember the order of insertion). If\n",
      "        ``object_hook`` is also defined, the ``object_pairs_hook`` takes priority.\n",
      "        \n",
      "        ``parse_float``, if specified, will be called with the string\n",
      "        of every JSON float to be decoded. By default this is equivalent to\n",
      "        float(num_str). This can be used to use another datatype or parser\n",
      "        for JSON floats (e.g. decimal.Decimal).\n",
      "        \n",
      "        ``parse_int``, if specified, will be called with the string\n",
      "        of every JSON int to be decoded. By default this is equivalent to\n",
      "        int(num_str). This can be used to use another datatype or parser\n",
      "        for JSON integers (e.g. float).\n",
      "        \n",
      "        ``parse_constant``, if specified, will be called with one of the\n",
      "        following strings: -Infinity, Infinity, NaN, null, true, false.\n",
      "        This can be used to raise an exception if invalid JSON numbers\n",
      "        are encountered.\n",
      "        \n",
      "        To use a custom ``JSONDecoder`` subclass, specify it with the ``cls``\n",
      "        kwarg; otherwise ``JSONDecoder`` is used.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['dump', 'dumps', 'load', 'loads', 'JSONDecoder', 'JSONEncod...\n",
      "    __author__ = 'Bob Ippolito <bob@redivi.com>'\n",
      "    __version__ = '2.0.9'\n",
      "\n",
      "VERSION\n",
      "    2.0.9\n",
      "\n",
      "AUTHOR\n",
      "    Bob Ippolito <bob@redivi.com>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[2.0', ', 1.0', ']']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ComplexEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, complex):\n",
    "            return [obj.real, obj.imag]\n",
    "        # Let the base class default method raise the TypeError\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "json.dumps(2 + 1j, cls=ComplexEncoder)\n",
    "ComplexEncoder().encode(2 + 1j)\n",
    "list(ComplexEncoder().iterencode(2 + 1j))\n",
    "\n",
    "# the following line I tried, but didn't\n",
    "# data = json.loads({'train.json'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = json.load(open('train.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'cuisine': u'greek', u'id': 10259, u'ingredients': [u'romaine lettuce', u'black olives', u'grape tomatoes', u'garlic', u'pepper', u'purple onion', u'seasoning', u'garbanzo beans', u'feta cheese crumbles']}\n"
     ]
    }
   ],
   "source": [
    "print data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'cuisine': u'greek', u'id': 10259, u'ingredients': [u'romaine lettuce', u'black olives', u'grape tomatoes', u'garlic', u'pepper', u'purple onion', u'seasoning', u'garbanzo beans', u'feta cheese crumbles']}, {u'cuisine': u'southern_us', u'id': 25693, u'ingredients': [u'plain flour', u'ground pepper', u'salt', u'tomatoes', u'ground black pepper', u'thyme', u'eggs', u'green tomatoes', u'yellow corn meal', u'milk', u'vegetable oil']}]\n"
     ]
    }
   ],
   "source": [
    "print data[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greek\n"
     ]
    }
   ],
   "source": [
    "print data[0]['cuisine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10259\n"
     ]
    }
   ],
   "source": [
    "print data[0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'romaine lettuce', u'black olives', u'grape tomatoes', u'garlic', u'pepper', u'purple onion', u'seasoning', u'garbanzo beans', u'feta cheese crumbles']\n"
     ]
    }
   ],
   "source": [
    "print data[0]['ingredients']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "romaine lettuce\n"
     ]
    }
   ],
   "source": [
    "print data[0]['ingredients'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-187-f8a48549e60a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cuisine'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers, not str"
     ]
    }
   ],
   "source": [
    "print data['cuisine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-188-e273c72e4da0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cuisine'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers, not str"
     ]
    }
   ],
   "source": [
    "print data[0::]['cuisine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-189-5f49c1576006>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cuisine'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers, not str"
     ]
    }
   ],
   "source": [
    "print data[0:2]['cuisine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "39774"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print type(data[0]['ingredients'])\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok so we have a list of dicts of lists ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to create a for loop, iterate through each item of the list, and gather a list of all of the ingredients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# my research reveals that sets (like set()) filter out duplicates, and lists don't. I'll make a set\n",
    "full_ingredient_set = set()\n",
    "\n",
    "count = 0\n",
    "for x in xrange (0,len(data)):\n",
    "    for y in xrange (0,len(data[x]['ingredients'])):\n",
    "        full_ingredient_set.add(data[x]['ingredients'][y])\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6714\n",
      "428275\n"
     ]
    }
   ],
   "source": [
    "print len(full_ingredient_set)\n",
    "print count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Great! Now we have a preliminary ingredient set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'set' object has no attribute '__getitem__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-195-05659a2b2d46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mfull_ingredient_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'set' object has no attribute '__getitem__'"
     ]
    }
   ],
   "source": [
    "print full_ingredient_set[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_ingredient_list = list(full_ingredient_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'low-sodium fat-free chicken broth', u'sweetened coconut', u'baking chocolate', u'egg roll wrappers', u'bottled low sodium salsa', u'vegan parmesan cheese', u'clam sauce', u'mahlab', u'(10 oz.) frozen chopped spinach, thawed and squeezed dry', u'figs', u'caramels', u'broiler', u'jalapeno chilies', u'(15 oz.) refried beans', u'brioche buns', u'broccoli romanesco', u'flaked oats', u'anise extract', u'whole wheat pastry flour', u'ravva']\n"
     ]
    }
   ],
   "source": [
    "print full_ingredient_list[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next let's get a list of the cuisine types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cuisine_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "for x in xrange (0,len(data)):\n",
    "    cuisine_set.add(data[x]['cuisine'])\n",
    "    \n",
    "print len(cuisine_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'irish', u'mexican', u'chinese', u'filipino', u'vietnamese', u'moroccan', u'brazilian', u'japanese', u'british', u'greek', u'indian', u'jamaican', u'french', u'spanish', u'russian', u'cajun_creole', u'thai', u'southern_us', u'korean', u'italian'])\n"
     ]
    }
   ],
   "source": [
    "print cuisine_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cuisine_list = list(cuisine_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'irish', u'mexican', u'chinese', u'filipino', u'vietnamese', u'moroccan', u'brazilian', u'japanese', u'british', u'greek', u'indian', u'jamaican', u'french', u'spanish', u'russian', u'cajun_creole', u'thai', u'southern_us', u'korean', u'italian']\n"
     ]
    }
   ],
   "source": [
    "print cuisine_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the 'u's for?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not sure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General data exploration: unique words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to see first if there are any words that don't overlap with words in any other list for another cuisine type, and then look at those words to see if they make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sets_of_ingredients = list(range(len(cuisine_list)))\n",
    "count = 0\n",
    "for x in xrange (0,len(data)):\n",
    "    for y in xrange (0, len(cuisine_list)):\n",
    "        sets_of_ingredients[y] = set()\n",
    "        for z in xrange (0,len(data[x]['ingredients'])):\n",
    "            sets_of_ingredients[y].add(data[x]['ingredients'][z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-204-17a60ed35f50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0munique_sets_ingredients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msets_of_ingredients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "unique_sets_ingredients = sets_of_ingredients.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that didn't work. Let's try this copy.deepcopy() function. Do we need to import copy first? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_sets_ingredients = copy.deepcopy(sets_of_ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy as copy #seems like yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_sets_ingredients = copy.deepcopy(sets_of_ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# So yeah, my plan is to go through each list, then each other list, \n",
    "# and if an item from one is in another, then delete it from both\n",
    "#\n",
    "# I'll use s.difference(t), which gives the elements of s that aren't in t. \n",
    "\n",
    "# also I'm tired of all of these calling len deals, so I'll come up with a new \n",
    "# variable for the number of cuisines\n",
    "\n",
    "n_cuisine = len(cuisine_set)\n",
    "\n",
    "for x in range (0,n_cuisine):\n",
    "    for y in range (0, n_cuisine):\n",
    "        if y != x:\n",
    "            unique_sets_ingredients[x] = sets_of_ingredients[x].difference(sets_of_ingredients[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "irish\n",
      "0\n",
      "mexican\n",
      "0\n",
      "chinese\n",
      "0\n",
      "filipino\n",
      "0\n",
      "vietnamese\n",
      "0\n",
      "moroccan\n",
      "0\n",
      "brazilian\n",
      "0\n",
      "japanese\n",
      "0\n",
      "british\n",
      "0\n",
      "greek\n",
      "0\n",
      "indian\n",
      "0\n",
      "jamaican\n",
      "0\n",
      "french\n",
      "0\n",
      "spanish\n",
      "0\n",
      "russian\n",
      "0\n",
      "cajun_creole\n",
      "0\n",
      "thai\n",
      "0\n",
      "southern_us\n",
      "0\n",
      "korean\n",
      "0\n",
      "italian\n"
     ]
    }
   ],
   "source": [
    "# Check if it worked (do we have smaller ingredient sets for each cuisine?)\n",
    "for x in range (0, n_cuisine):\n",
    "    print len(unique_sets_ingredients[x])\n",
    "    print cuisine_list[x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok, it's hard to say if it worked or not. \n",
    "Maybe every ingredient was on there at least once? Or I easily could have messed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test by adding in a nonsensical string as an ingredient to test\n",
    "unique_sets_ingredients_test = copy.deepcopy(sets_of_ingredients)\n",
    "unique_sets_ingredients_test[0].add('thisisatest')\n",
    "\n",
    "for x in range (0,n_cuisine):\n",
    "    for y in range (0, n_cuisine):\n",
    "        if y != x:\n",
    "            unique_sets_ingredients_test[x] = \\\n",
    "                unique_sets_ingredients_test[x].difference(unique_sets_ingredients_test[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thisisatest'}"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if it worked (is my test string in there?)\n",
    "unique_sets_ingredients_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, seems to have worked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique ingredients sets will not work, then. Bummer. New strategy!\n",
    "Let's look at the lists individually and see what we're working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'ground black pepper', u'salt', u'dried oregano', u'celery', u'garlic', u'chopped cilantro fresh', u'white sugar', u'jalapeno chilies', u'green chile', u'green bell pepper', u'onions', u'roma tomatoes'])\n"
     ]
    }
   ],
   "source": [
    "print sets_of_ingredients[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait - I think I messed up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above looks like the ingredient list of just one recipe. I think I just looked if the first twenty recipes had any ingredients that the other ones didn't have. I don't know if I ever consolidated the lists for each cuisine type. In fact, I know I didn't. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'ground black pepper', u'salt', u'dried oregano', u'celery', u'garlic', u'chopped cilantro fresh', u'white sugar', u'jalapeno chilies', u'green chile', u'green bell pepper', u'onions', u'roma tomatoes'])\n"
     ]
    }
   ],
   "source": [
    "print sets_of_ingredients[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'ground black pepper', u'salt', u'dried oregano', u'celery', u'garlic', u'chopped cilantro fresh', u'white sugar', u'jalapeno chilies', u'green chile', u'green bell pepper', u'onions', u'roma tomatoes'])\n"
     ]
    }
   ],
   "source": [
    "# What is this list even? \n",
    "print sets_of_ingredients[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just a repeat of the same list\n",
    "I obviously messed up somewhere. Just need to find where. \n",
    "\n",
    "Ha, just. As if. Step 1 is find where, then I'll be back at square 1 essentially. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-215-897c97ce094e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0msets_of_ingredients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print sets_of_ingredients[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, this list is messed up. \n",
    "\n",
    "When did I create it? - started with sets_of_ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39774"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following code is what I originally ran to create sets_of_lists\n",
    "\n",
    "#\n",
    "# sets_of_ingredients = list(range(len(cuisine_list)))\n",
    "# count = 0\n",
    "# for x in xrange (0,len(data)):\n",
    "#     for y in xrange (0, len(cuisine_list)):\n",
    "#         sets_of_ingredients[y] = set()\n",
    "#         for z in xrange (0,len(data[x]['ingredients'])):\n",
    "#             sets_of_ingredients[y].add(data[x]['ingredients'][z])\n",
    "#            \n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the error above?\n",
    "Let's find it together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'union'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-217-840e33128471>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# New stuff. Check if cuisine matches recipe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcuisine_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cuisine'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0msets_of_ingredients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msets_of_ingredients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ingredients'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'union'"
     ]
    }
   ],
   "source": [
    "# first line - looks good, one set for each cuisine type\n",
    "sets_of_ingredients = list(range(len(cuisine_list)))\n",
    "\n",
    "# Set up outer loop - looks good, will go through each recipe\n",
    "for x in xrange (0,len(data)):\n",
    "    # Set up a second loop to go through cuisine types, I think...\n",
    "    for y in xrange (0, len(cuisine_list)):\n",
    "        # New stuff. Check if cuisine matches recipe\n",
    "        if cuisine_list[y] == data[x]['cuisine']:\n",
    "            sets_of_ingredients[y] = sets_of_ingredients[y].union(set(data[x]['ingredients']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Nevermind, that won't work because I couldn't figure out how to initialize \n",
    "# the parts of list as sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_cuisine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sets_of_ingredients = list(range(0, n_cuisine))\n",
    "\n",
    "# Initialize each item in list as a set\n",
    "for x in xrange (0, n_cuisine):\n",
    "    sets_of_ingredients[x] = set()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up outer loop to go through each recipe\n",
    "for x in xrange (0,len(data)):\n",
    "    # Get the cuisine index\n",
    "    index = cuisine_list.index(data[x]['cuisine'])\n",
    "    # Add new ingredients to existing set\n",
    "    sets_of_ingredients[index] = sets_of_ingredients[index].union(set(data[x]['ingredients']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so the above lines of code took a few seconds to run, so that's a good sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now run those same lines from before to see if there is any cuisine\n",
    "# ingredient overlap\n",
    "\n",
    "unique_sets_ingredients = copy.deepcopy(sets_of_ingredients)\n",
    "\n",
    "for x in range (0,n_cuisine):\n",
    "    for y in range (0, n_cuisine):\n",
    "        if y != x:\n",
    "            unique_sets_ingredients[x] = sets_of_ingredients[x].difference(sets_of_ingredients[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'banger', u'candied peel', u'oat bran', u'sweetened coconut', u'ale', u'marshmallow creme', u'chopped potatoes', u'pico de gallo', u'fat-free buttermilk', u'caster', u'dried lavender', u'kiwi fruits', u'maraschino', u'self raising flour', u'gala apples', u'cranberry juice cocktail', u'raw honey', u'treacle', u'raspberry preserves', u'tartar sauce', u'split peas', u'Jameson Irish Whiskey', u'steak sauce', u'sparkling sugar', u'hot tea', u'whole cloves', u'maple sugar', u'wholemeal flour', u'caul fat', u'ginger ale', u'irish oats', u'pork butt', u'Baileys Irish Cream Liqueur', u'suet', u'cardamom seeds', u'chutney', u'coriander', u'biscuit mix', u'tenderloin roast', u'instant pudding mix', u'xanthan gum', u'teas', u'cider', u'scones', u'dried plum', u'hot mustard', u\"pig's trotters\", u'condensed cream', u'vanilla wafer crumbs', u'lager', u'lamb loin', u'pickle relish', u'sultana', u'mint chocolate chip ice cream', u'liquor', u'virginia ham', u'meringue powder', u'dried pear', u'pastry flour', u\"M&M's Candy\", u'rye whiskey', u'cura\\xe7ao', u'pork blood', u'bacon drippings', u'chicken gravy mix', u'black pudding', u'creamy peanut butter', u'young nettle', u'mutton', u'sauerkraut', u'apple juice concentrate', u'vegan bouillon cubes', u'knorr leek recip mix', u'table wine', u'back bacon', u'tart shells', u'sourdough starter', u'mashed potatoes', u'fudge brownie mix', u'sliced leeks', u'lean steak', u'dark muscovado sugar', u'porter', u'cointreau liqueur', u'irish bacon', u'OREO\\xae Cookies', u'pumpernickel bread', u'persian cucumber', u'yellow peas', u'brewed tea', u'black tea', u'gluten-free flour', u'lamb chops', u'pickled jalapenos', u'frozen orange juice concentrate', u'chicken carcass', u'fillet medallions', u'lamb leg', u'muesli', u'dates', u'dried apple', u'sugar pumpkin', u'cinnamon sugar', u'morsels', u'oatmeal', u'vanilla lowfat yogurt', u'marmalade', u'prepared coleslaw', u'bologna', u'smoked haddock', u'mandarin orange segments', u'English mustard', u'peanut butter', u'stew', u'lamb cutlet', u'rib-eye roast', u'milk chocolate chips', u'london broil', u'malt vinegar', u'flax seed meal', u'decorating sugars', u'stew meat', u'potato bread', u'large marshmallows', u'sorghum flour', u'rounds', u'custard', u'cubed meat', u'light molasses', u'rutabaga', u'extra fine granulated sugar', u'gluten-free broth', u'superfine white sugar', u'brisket', u'whole allspice', u'sour cherries', u'slaw', u'cream cheese frosting', u'Challenge Butter', u'ginger beer', u'cream sauce', u'extra sharp white cheddar cheese', u'Jameson Whiskey', u'toasted coconut', u'pinhead oatmeal', u'curly kale', u'lamb for stew', u'cereal', u'cooked barley', u'stewing steak', u'Kerrygold Pure Irish Butter', u'soda bread', u'pickles', u'fresh cranberries', u'graham flour', u'double-acting baking powder', u'dark beer', u'french fries', u'pie dough', u'citrus fruit', u'butterscotch filling', u'frozen peas and carrots', u'golden delicious apples', u'cipollini', u'single crust pie', u'unsweetened apple juice', u'Guinness Beer', u'rainbow trout', u'steel-cut oatmeal', u'baby potatoes', u'frozen mashed potatoes', u'pork hocks', u'mixed peel', u'cooking apples', u'chervil', u'chocolate cookie crumbs', u'kumquats in syrup', u'beef jerky', u'organic chicken broth', u'lamb neck', u'Country Crock\\xae Spread', u'muscovado sugar', u'sprite', u'pot roast', u'shortcrust pastry', u'dried fruit', u'dried chives', u'beef rib roast', u'bacon salt', u'piecrust', u'rock salt', u'semisweet baking chocolate', u'gingersnap crumbs', u'chocolate candy', u'jimmies', u'gravy', u'papaya', u'clear honey', u'dessert wine', u'cardamom', u'Guinness Lager', u'Irish whiskey', u'stewing beef', u'mixed spice', u'flat cut', u'Irish Red ale', u'potato flakes', u'croissants', u'dark molasses', u'dried strawberries', u'rolled oats', u'bicarbonate of soda', u'Japanese turnips', u'caraway', u'lamb stock', u'fresh ginger root', u'beef sirloin', u'corn flour', u'arrowroot powder', u'savoy cabbage leaves', u'bacon grease', u'popped popcorn', u'prime rib', u'pudding powder', u'fine salt', u'cr\\xe8me de menthe', u'rum extract'])\n"
     ]
    }
   ],
   "source": [
    "print unique_sets_ingredients[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n",
      "227\n"
     ]
    }
   ],
   "source": [
    "print len(sets_of_ingredients[0])\n",
    "print len(unique_sets_ingredients[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "irish\n",
      "227 999\n",
      "mexican\n",
      "1055 2684\n",
      "chinese\n",
      "748 1792\n",
      "filipino\n",
      "311 947\n",
      "vietnamese\n",
      "404 1108\n",
      "moroccan\n",
      "187 974\n",
      "brazilian\n",
      "203 853\n",
      "japanese\n",
      "568 1439\n",
      "british\n",
      "320 1166\n",
      "greek\n",
      "207 1198\n",
      "indian\n",
      "607 1664\n",
      "jamaican\n",
      "215 877\n",
      "french\n",
      "564 2102\n",
      "spanish\n",
      "223 1263\n",
      "russian\n",
      "180 872\n",
      "cajun_creole\n",
      "396 1576\n",
      "thai\n",
      "498 1376\n",
      "southern_us\n",
      "911 2462\n",
      "korean\n",
      "313 898\n",
      "italian\n",
      "2344 2929\n",
      "10481\n"
     ]
    }
   ],
   "source": [
    "unique_ingredients_total = 0\n",
    "\n",
    "# Check if it worked (do we have smaller ingredient sets for each cuisine?)\n",
    "for x in range (0, n_cuisine):\n",
    "    print cuisine_list[x]\n",
    "    print len(unique_sets_ingredients[x]), len(sets_of_ingredients[x])\n",
    "    unique_ingredients_total += len(unique_sets_ingredients[x])\n",
    "print unique_ingredients_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total of the unique ingredients is more than the actual total of unique ingredients that I calculated earlier, so something is defiitely wrong, but i'm not sure what. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(unique_sets_ingredients[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'lager', u'oat bran', u'marshmallow creme', u'stew meat', u'pickle relish', u'muscovado sugar', u'tenderloin roast', u'malt vinegar', u'slaw', u'ale', u'decorating sugars', u'milk chocolate chips', u'french fries', u'biscuit mix', u'Country Crock\\xae Spread', u'pico de gallo', u'fat-free buttermilk', u'rye whiskey', u'beef sirloin', u'organic chicken broth', u'bacon drippings', u'pickles', u'kiwi fruits', u'ginger beer', u'lean steak', u'brisket', u'whole allspice', u'mashed potatoes', u'fudge brownie mix', u'shortcrust pastry', u'steak sauce', u'whole cloves', u'gravy', u'papaya', u'pumpernickel bread', u'cream sauce', u'extra sharp white cheddar cheese', u'single crust pie', u'pot roast', u'black tea', u'gluten-free flour', u'pork butt', u'fine salt', u'pickled jalapenos', u'frozen orange juice concentrate', u'mixed spice', u'stew', u'flax seed meal', u'potato flakes', u'dark molasses', u'chutney', u'rolled oats', u'coriander', u'fresh cranberries', u'fresh ginger root', u'xanthan gum', u'tartar sauce', u'teas', u'dark beer', u'corn flour', u'curly kale', u'pie dough', u'creamy peanut butter', u'bacon grease', u'stewing beef', u'popped popcorn', u'cinnamon sugar', u'arrowroot powder', u'unsweetened apple juice', u'mandarin orange segments', u'peanut butter'])\n"
     ]
    }
   ],
   "source": [
    "print unique_sets_ingredients[0].intersection(unique_sets_ingredients[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One more try\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now run those same lines from before to see if there is any cuisine\n",
    "# ingredient overlap\n",
    "\n",
    "unique_sets_ingredients = copy.deepcopy(sets_of_ingredients)\n",
    "\n",
    "for x in range (0,n_cuisine):\n",
    "    for y in range (0, n_cuisine):\n",
    "        if y != x:\n",
    "            unique_sets_ingredients[x] = unique_sets_ingredients[x].difference(unique_sets_ingredients[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "print len(sets_of_ingredients[0])\n",
    "print len(unique_sets_ingredients[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "irish\n",
      "36 999\n",
      "mexican\n",
      "457 2684\n",
      "chinese\n",
      "209 1792\n",
      "filipino\n",
      "67 947\n",
      "vietnamese\n",
      "78 1108\n",
      "moroccan\n",
      "39 974\n",
      "brazilian\n",
      "71 853\n",
      "japanese\n",
      "188 1439\n",
      "british\n",
      "97 1166\n",
      "greek\n",
      "90 1198\n",
      "indian\n",
      "256 1664\n",
      "jamaican\n",
      "72 877\n",
      "french\n",
      "288 2102\n",
      "spanish\n",
      "103 1263\n",
      "russian\n",
      "95 872\n",
      "cajun_creole\n",
      "199 1576\n",
      "thai\n",
      "290 1376\n",
      "southern_us\n",
      "837 2462\n",
      "korean\n",
      "313 898\n",
      "italian\n",
      "2929 2929\n",
      "6714\n"
     ]
    }
   ],
   "source": [
    "# Check to see if each list is much smaller\n",
    "\n",
    "# Total up the num of ingredients just in one list\n",
    "unique_ingredients_total_2 = 0\n",
    "\n",
    "for x in range (0, n_cuisine):\n",
    "    print cuisine_list[x]\n",
    "    print len(unique_sets_ingredients[x]), len(sets_of_ingredients[x])\n",
    "    unique_ingredients_total_2 += len(unique_sets_ingredients[x])\n",
    "print unique_ingredients_total_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Well, something is still wrong, since we have 6714 as the total, which was the original total (and italian says none of its ingredients overlap with others)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third time's a charm\n",
    "Last time I was checking to see how many of the ingredients of one unique set list overlapped with other unique set lists, instead of how many overlapped with other set lists generally (because over time, the unique set lists were growing smaller and smaller). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now run those same lines from before to see if there is any cuisine\n",
    "# ingredient overlap\n",
    "\n",
    "unique_sets_ingredients = copy.deepcopy(sets_of_ingredients)\n",
    "\n",
    "for x in range (0,n_cuisine):\n",
    "    for y in range (0, n_cuisine):\n",
    "        if y != x:\n",
    "            unique_sets_ingredients[x] = unique_sets_ingredients[x].difference(sets_of_ingredients[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "irish\n",
      "36 999\n",
      "mexican\n",
      "456 2684\n",
      "chinese\n",
      "197 1792\n",
      "filipino\n",
      "51 947\n",
      "vietnamese\n",
      "46 1108\n",
      "moroccan\n",
      "34 974\n",
      "brazilian\n",
      "50 853\n",
      "japanese\n",
      "139 1439\n",
      "british\n",
      "76 1166\n",
      "greek\n",
      "65 1198\n",
      "indian\n",
      "143 1664\n",
      "jamaican\n",
      "36 877\n",
      "french\n",
      "193 2102\n",
      "spanish\n",
      "54 1263\n",
      "russian\n",
      "49 872\n",
      "cajun_creole\n",
      "105 1576\n",
      "thai\n",
      "74 1376\n",
      "southern_us\n",
      "272 2462\n",
      "korean\n",
      "41 898\n",
      "italian\n",
      "480 2929\n",
      "2597\n"
     ]
    }
   ],
   "source": [
    "# Check to see if each list is much smaller\n",
    "\n",
    "# Total up the num of ingredients just in one list\n",
    "unique_ingredients_total_3 = 0\n",
    "\n",
    "for x in range (0, n_cuisine):\n",
    "    print cuisine_list[x]\n",
    "    print len(unique_sets_ingredients[x]), len(sets_of_ingredients[x])\n",
    "    unique_ingredients_total_3 += len(unique_sets_ingredients[x])\n",
    "print unique_ingredients_total_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Presumed) Success!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2597 of the original 6714 ingredients are unique to one particular type of cuisine. Can I use that to characterize the new ones? It seems like a really dumb way to do it, but why not try? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'Hidden Valley\\xae Greek Yogurt Original Ranch\\xae Dip Mix', u'honey-flavored greek style yogurt', u'grated kefalotiri', u'Cavenders Greek Seasoning', u'pork tenderloin medallions', u'sunflower kernels', u'ammonium bicarbonate', u'tarama', u'Greek dressing', u'mahimahi fillet', u'goat milk feta', u'low-fat caesar dressing', u'pocket bread', u'lamb steaks', u'red wine vinaigrette', u'boneless skinless chicken thigh fillets', u'kasseri', u'wish bone red wine vinaigrett dress', u'grape vine leaves', u'dillweed', u'yogurt low fat', u'Greek feta', u'Stonefire Tandoori Garlic Naan', u'aleppo', u'lowfat plain greekstyl yogurt', u'myzithra', u'Mezzetta Sliced Greek Kalamata Olives', u'Greek black olives', u'lean minced lamb', u'Yoplait\\xae Greek 2% caramel yogurt', u'Homemade Yogurt', u'pointed peppers', u'raki', u'kefalotyri', u'whole wheat pita pockets', u'low-fat feta', u'fresh spinach leaves, rins and pat dry', u'mahlab', u'pita wedges', u'pita wraps', u'curly leaf spinach', u'manouri', u'lamb rib roast', u'ouzo', u'baked pita chips', u'graviera', u'low-fat plain greek yogurt', u'rusk', u'greek style seasoning', u'whole wheat pita bread rounds', u'Mazola Canola Oil', u'dried apple rings', u'frozen basil', u'shank half', u'sliced kalamata olives', u'Betty Crocker\\u2122 oatmeal cookie mix', u'greek seasoning mix', u'low-fat white sauce', u'greek-style vinaigrette', u'Yoplait\\xae Greek 100 blackberry pie yogurt', u'watermelon seeds', u'butter-flavored spray', u'kalamata olive halves', u'2% lowfat greek yogurt', u'pita loaves'])\n"
     ]
    }
   ],
   "source": [
    "# first look at examples of these unique food lists\n",
    "# Keep in mind could be misspellings of words, brands, duplicates because of capitalization\n",
    "\n",
    "print unique_sets_ingredients[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without even looking at the name, it definitely looks like greek food. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load test data!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if I remember how to load this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in test data\n",
    "test_data = json.load(open('test.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No error message, so far so good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "{u'id': 18009, u'ingredients': [u'baking powder', u'eggs', u'all-purpose flour', u'raisins', u'milk', u'white sugar']}\n"
     ]
    }
   ],
   "source": [
    "# Look at first entry in test data and data type\n",
    "print type(test_data)\n",
    "print test_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, probably the same format as the train data, a list of dictionaries of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'dict'>\n",
      "<type 'list'>\n"
     ]
    }
   ],
   "source": [
    "print type(test_data[0])\n",
    "print type(test_data[0]['ingredients'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, it's the same, just no cuisine type label. \n",
    "\n",
    "Not entirely sure how we're going to go about labeling this data, but let's give it a go. \n",
    "\n",
    "Remember, if we label everything as entirely italian, we know we'll get something like 20% accuracy, so we want to beat that. And if we fail to label anything, we can call it 'italian' and be done with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we will want to make a csv document that has the id number and the label, I presume. let's load the example submission file to make sure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'load'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-238-5be3159436df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample_submission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sample_submission.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'load'"
     ]
    }
   ],
   "source": [
    "sample_submission = csv.load('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that's not a thing. I'll go back and look at my Titanic solution and see how we load csv files into Python. BRB. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "csv_reader = csv.reader(open('sample_submission.csv', 'rb'))\n",
    "header = csv_reader.next()\n",
    "\n",
    "# Create variable 'data'\n",
    "# Go through each row of csv file, add to data\n",
    "sample_submission = []\n",
    "for row in csv_reader: \n",
    "    sample_submission.append(row)\n",
    "    \n",
    "# convert from list to array\n",
    "sample_submission = np.array(sample_submission)\n",
    "\n",
    "# each item is currently a string format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['35203', 'italian'],\n",
       "       ['17600', 'italian'],\n",
       "       ['35200', 'italian'],\n",
       "       ..., \n",
       "       ['15430', 'italian'],\n",
       "       ['46530', 'italian'],\n",
       "       ['30849', 'italian']], \n",
       "      dtype='|S7')"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, it is as I expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print type(sample_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print type(sample_submission[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a numpy array of numpy arrays. I could make one of those for the test data pretty easily. just have to iterate through it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan of attack\n",
    "So my plan of attack is to get the length of the test data, iterate through it, check to see if any of the words in the unique words lists are in there, and if they are, pick the cuisine type of the most common unique word. And if there aren't any, choose italian. Then build a numpy array out of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9944\n"
     ]
    }
   ],
   "source": [
    "# Get length of test data set\n",
    "n_test = len(test_data)\n",
    "print n_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, step 1 complete. Next, figure out best way to keep track of the word counts. I can think of 2 ways this might work - 1 is to count the most common of the unique words and pick that word's type. Or we could pick the cuisine type with the most of the unique words. Or the cuisine type with the most instances of the unique words (like total number of times for any unique words). I wonder how different those outcomes would be? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First try - cuisine type of most common unique word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, I just realized I'm being an idiot. None of the words in a single recipe are going to show up more than once. \n",
    "\n",
    "Ok so new plan - for each recipe, make a count of the number of unique words from each cuisine that are being considered, and pick the one with the highest count. If none are included, pick italian. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18009, 'italian']\n",
      "1475\n",
      "1281\n"
     ]
    }
   ],
   "source": [
    "submission_data = []\n",
    "\n",
    "test_count = 0\n",
    "test_count_2 = 0\n",
    "# Iterate through test data\n",
    "for x in xrange(0, n_test):\n",
    "    # Make list of unique word counts\n",
    "    counts = [0] * n_test\n",
    "    \n",
    "    # iterate through each ingredient in the recipe - not sure if this will work\n",
    "    for ingredient in test_data[x]['ingredients']:\n",
    "        \n",
    "        # iterate through each cuisine type\n",
    "        for y in xrange(0, n_cuisine):\n",
    "        \n",
    "            # if the ingredient is in one of the unique words lists, add one to counts[y]\n",
    "            if ingredient in unique_sets_ingredients[y]:\n",
    "                counts[y] += 1 \n",
    "                test_count += 1\n",
    "                \n",
    "    # Find the max of counts\n",
    "    max_count = max(counts)\n",
    "    \n",
    "    if max_count:\n",
    "        label = cuisine_list[counts.index(max_count)]\n",
    "        test_count_2 += 1\n",
    "    else:\n",
    "        label = 'italian'\n",
    "        \n",
    "    # Add results to the array\n",
    "    temp_array = [test_data[x]['id'], label]\n",
    "    submission_data.append(temp_array)\n",
    "    \n",
    "print submission_data[0]\n",
    "print test_count\n",
    "print test_count_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28583, 'italian']\n"
     ]
    }
   ],
   "source": [
    "print submission_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18009, 'italian'], [28583, 'italian'], [41580, 'italian'], [29752, 'italian'], [35687, 'italian'], [38527, 'italian'], [19666, u'italian'], [41217, 'italian'], [28753, u'mexican'], [22659, 'italian'], [21749, 'italian'], [44967, u'southern_us'], [42969, 'italian'], [44883, 'italian'], [20827, 'italian'], [23196, 'italian'], [35387, 'italian'], [33780, 'italian'], [19001, 'italian'], [16526, 'italian'], [42455, 'italian'], [47453, 'italian'], [42478, 'italian'], [11885, u'vietnamese'], [16585, 'italian'], [29639, 'italian'], [26245, 'italian'], [38516, 'italian'], [47520, 'italian'], [26212, u'cajun_creole'], [23696, 'italian'], [14926, 'italian'], [13292, 'italian'], [27346, 'italian'], [1384, 'italian'], [15959, 'italian'], [42297, 'italian'], [46235, 'italian'], [21181, 'italian'], [9809, 'italian']]\n"
     ]
    }
   ],
   "source": [
    "print submission_data[0:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well obviously that failed, but at least I have a set where they are all italian and I can at least see what the result is and submit it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "italian\n",
      "mexican\n",
      "southern_us\n",
      "vietnamese\n",
      "cajun_creole\n",
      "japanese\n",
      "chinese\n",
      "filipino\n",
      "french\n",
      "indian\n",
      "mexican\n",
      "french\n",
      "mexican\n",
      "thai\n",
      "vietnamese\n",
      "vietnamese\n",
      "southern_us\n",
      "mexican\n",
      "mexican\n",
      "indian\n",
      "brazilian\n",
      "southern_us\n",
      "chinese\n",
      "mexican\n",
      "japanese\n",
      "moroccan\n",
      "mexican\n",
      "mexican\n",
      "southern_us\n",
      "korean\n",
      "mexican\n",
      "mexican\n",
      "irish\n",
      "mexican\n",
      "french\n",
      "korean\n",
      "british\n",
      "indian\n",
      "mexican\n",
      "indian\n",
      "southern_us\n",
      "mexican\n",
      "thai\n",
      "japanese\n",
      "vietnamese\n",
      "southern_us\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "moroccan\n",
      "mexican\n",
      "russian\n",
      "filipino\n",
      "british\n",
      "mexican\n",
      "chinese\n",
      "mexican\n",
      "korean\n",
      "brazilian\n",
      "korean\n",
      "mexican\n",
      "chinese\n",
      "mexican\n",
      "southern_us\n",
      "indian\n",
      "mexican\n",
      "mexican\n",
      "indian\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "indian\n",
      "chinese\n",
      "indian\n",
      "japanese\n",
      "mexican\n",
      "french\n",
      "mexican\n",
      "mexican\n",
      "southern_us\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "chinese\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "japanese\n",
      "chinese\n",
      "british\n",
      "indian\n",
      "japanese\n",
      "mexican\n",
      "vietnamese\n",
      "mexican\n",
      "brazilian\n",
      "mexican\n",
      "southern_us\n",
      "mexican\n",
      "mexican\n",
      "french\n",
      "korean\n",
      "southern_us\n",
      "mexican\n",
      "chinese\n",
      "mexican\n",
      "southern_us\n",
      "chinese\n",
      "russian\n",
      "chinese\n",
      "chinese\n",
      "british\n",
      "greek\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "indian\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "chinese\n",
      "mexican\n",
      "greek\n",
      "mexican\n",
      "mexican\n",
      "filipino\n",
      "spanish\n",
      "spanish\n",
      "chinese\n",
      "greek\n",
      "indian\n",
      "indian\n",
      "thai\n",
      "mexican\n",
      "japanese\n",
      "thai\n",
      "chinese\n",
      "indian\n",
      "japanese\n",
      "mexican\n",
      "french\n",
      "southern_us\n",
      "french\n",
      "chinese\n",
      "southern_us\n",
      "mexican\n",
      "mexican\n",
      "indian\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "korean\n",
      "southern_us\n",
      "mexican\n",
      "chinese\n",
      "jamaican\n",
      "chinese\n",
      "mexican\n",
      "chinese\n",
      "filipino\n",
      "japanese\n",
      "chinese\n",
      "mexican\n",
      "brazilian\n",
      "chinese\n",
      "brazilian\n",
      "japanese\n",
      "indian\n",
      "mexican\n",
      "japanese\n",
      "southern_us\n",
      "jamaican\n",
      "southern_us\n",
      "indian\n",
      "british\n",
      "mexican\n",
      "japanese\n",
      "french\n",
      "jamaican\n",
      "korean\n",
      "mexican\n",
      "filipino\n",
      "brazilian\n",
      "indian\n",
      "mexican\n",
      "chinese\n",
      "british\n",
      "indian\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "japanese\n",
      "southern_us\n",
      "mexican\n",
      "korean\n",
      "korean\n",
      "japanese\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "chinese\n",
      "southern_us\n",
      "mexican\n",
      "mexican\n",
      "southern_us\n",
      "mexican\n",
      "mexican\n",
      "filipino\n",
      "french\n",
      "mexican\n",
      "southern_us\n",
      "brazilian\n",
      "filipino\n",
      "greek\n",
      "brazilian\n",
      "chinese\n",
      "mexican\n",
      "chinese\n",
      "cajun_creole\n",
      "mexican\n",
      "mexican\n",
      "spanish\n",
      "french\n",
      "thai\n",
      "french\n",
      "chinese\n",
      "russian\n",
      "mexican\n",
      "brazilian\n",
      "mexican\n",
      "indian\n",
      "japanese\n",
      "southern_us\n",
      "mexican\n",
      "mexican\n",
      "southern_us\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "brazilian\n",
      "mexican\n",
      "mexican\n",
      "indian\n",
      "jamaican\n",
      "mexican\n",
      "mexican\n",
      "indian\n",
      "mexican\n",
      "southern_us\n",
      "mexican\n",
      "mexican\n",
      "vietnamese\n",
      "french\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "brazilian\n",
      "indian\n",
      "chinese\n",
      "southern_us\n",
      "french\n",
      "mexican\n",
      "indian\n",
      "russian\n",
      "indian\n",
      "mexican\n",
      "japanese\n",
      "french\n",
      "mexican\n",
      "mexican\n",
      "greek\n",
      "korean\n",
      "mexican\n",
      "indian\n",
      "french\n",
      "mexican\n",
      "mexican\n",
      "french\n",
      "greek\n",
      "cajun_creole\n",
      "mexican\n",
      "indian\n",
      "brazilian\n",
      "mexican\n",
      "french\n",
      "russian\n",
      "mexican\n",
      "japanese\n",
      "cajun_creole\n",
      "mexican\n",
      "mexican\n",
      "french\n",
      "mexican\n",
      "mexican\n",
      "jamaican\n",
      "japanese\n",
      "mexican\n",
      "indian\n",
      "jamaican\n",
      "korean\n",
      "vietnamese\n",
      "mexican\n",
      "vietnamese\n",
      "mexican\n",
      "chinese\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "southern_us\n",
      "mexican\n",
      "filipino\n",
      "irish\n",
      "mexican\n",
      "mexican\n",
      "cajun_creole\n",
      "indian\n",
      "mexican\n",
      "british\n",
      "brazilian\n",
      "mexican\n",
      "chinese\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "cajun_creole\n",
      "french\n",
      "mexican\n",
      "french\n",
      "mexican\n",
      "brazilian\n",
      "cajun_creole\n",
      "chinese\n",
      "cajun_creole\n",
      "filipino\n",
      "british\n",
      "mexican\n",
      "greek\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "chinese\n",
      "indian\n",
      "southern_us\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "indian\n",
      "french\n",
      "mexican\n",
      "mexican\n",
      "japanese\n",
      "indian\n",
      "mexican\n",
      "mexican\n",
      "greek\n",
      "southern_us\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "greek\n",
      "southern_us\n",
      "spanish\n",
      "chinese\n",
      "mexican\n",
      "mexican\n",
      "chinese\n",
      "korean\n",
      "brazilian\n",
      "japanese\n",
      "french\n",
      "jamaican\n",
      "mexican\n",
      "mexican\n",
      "cajun_creole\n",
      "brazilian\n",
      "southern_us\n",
      "mexican\n",
      "mexican\n",
      "brazilian\n",
      "southern_us\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "japanese\n",
      "mexican\n",
      "indian\n",
      "moroccan\n",
      "french\n",
      "mexican\n",
      "mexican\n",
      "southern_us\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "cajun_creole\n",
      "southern_us\n",
      "mexican\n",
      "southern_us\n",
      "southern_us\n",
      "mexican\n",
      "brazilian\n",
      "spanish\n",
      "french\n",
      "southern_us\n",
      "mexican\n",
      "japanese\n",
      "mexican\n",
      "southern_us\n",
      "mexican\n",
      "chinese\n",
      "thai\n",
      "mexican\n",
      "cajun_creole\n",
      "mexican\n",
      "korean\n",
      "southern_us\n",
      "indian\n",
      "mexican\n",
      "chinese\n",
      "mexican\n",
      "brazilian\n",
      "mexican\n",
      "southern_us\n",
      "mexican\n",
      "filipino\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "irish\n",
      "mexican\n",
      "chinese\n",
      "southern_us\n",
      "french\n",
      "chinese\n",
      "japanese\n",
      "jamaican\n",
      "mexican\n",
      "mexican\n",
      "vietnamese\n",
      "brazilian\n",
      "thai\n",
      "french\n",
      "mexican\n",
      "mexican\n",
      "korean\n",
      "chinese\n",
      "mexican\n",
      "french\n",
      "chinese\n",
      "brazilian\n",
      "french\n",
      "mexican\n",
      "thai\n",
      "french\n",
      "mexican\n",
      "japanese\n",
      "thai\n",
      "vietnamese\n",
      "mexican\n",
      "cajun_creole\n",
      "chinese\n",
      "southern_us\n",
      "indian\n",
      "russian\n",
      "mexican\n",
      "mexican\n",
      "japanese\n",
      "chinese\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "indian\n",
      "french\n",
      "chinese\n",
      "southern_us\n",
      "filipino\n",
      "mexican\n",
      "french\n",
      "vietnamese\n",
      "brazilian\n",
      "chinese\n",
      "french\n",
      "french\n",
      "indian\n",
      "southern_us\n",
      "mexican\n",
      "japanese\n",
      "mexican\n",
      "brazilian\n",
      "southern_us\n",
      "mexican\n",
      "cajun_creole\n",
      "irish\n",
      "russian\n",
      "chinese\n",
      "mexican\n",
      "mexican\n",
      "indian\n",
      "mexican\n",
      "jamaican\n",
      "japanese\n",
      "moroccan\n",
      "japanese\n",
      "thai\n",
      "chinese\n",
      "indian\n",
      "mexican\n",
      "korean\n",
      "mexican\n",
      "mexican\n",
      "japanese\n",
      "mexican\n",
      "vietnamese\n",
      "southern_us\n",
      "southern_us\n",
      "mexican\n",
      "mexican\n",
      "filipino\n",
      "southern_us\n",
      "mexican\n",
      "mexican\n",
      "thai\n",
      "chinese\n",
      "thai\n",
      "mexican\n",
      "filipino\n",
      "french\n",
      "greek\n",
      "indian\n",
      "chinese\n",
      "vietnamese\n",
      "mexican\n",
      "chinese\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "irish\n",
      "mexican\n",
      "mexican\n",
      "greek\n",
      "chinese\n",
      "southern_us\n",
      "mexican\n",
      "mexican\n",
      "southern_us\n",
      "thai\n",
      "greek\n",
      "russian\n",
      "chinese\n",
      "french\n",
      "mexican\n",
      "korean\n",
      "indian\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "indian\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "southern_us\n",
      "mexican\n",
      "mexican\n",
      "chinese\n",
      "french\n",
      "moroccan\n",
      "southern_us\n",
      "british\n",
      "mexican\n",
      "jamaican\n",
      "korean\n",
      "spanish\n",
      "brazilian\n",
      "cajun_creole\n",
      "southern_us\n",
      "southern_us\n",
      "cajun_creole\n",
      "indian\n",
      "filipino\n",
      "mexican\n",
      "brazilian\n",
      "mexican\n",
      "mexican\n",
      "british\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "indian\n",
      "mexican\n",
      "chinese\n",
      "jamaican\n",
      "mexican\n",
      "southern_us\n",
      "japanese\n",
      "mexican\n",
      "indian\n",
      "southern_us\n",
      "southern_us\n",
      "japanese\n",
      "southern_us\n",
      "mexican\n",
      "british\n",
      "indian\n",
      "brazilian\n",
      "chinese\n",
      "mexican\n",
      "southern_us\n",
      "russian\n",
      "filipino\n",
      "indian\n",
      "mexican\n",
      "jamaican\n",
      "british\n",
      "korean\n",
      "jamaican\n",
      "mexican\n",
      "mexican\n",
      "indian\n",
      "filipino\n",
      "southern_us\n",
      "british\n",
      "jamaican\n",
      "vietnamese\n",
      "southern_us\n",
      "cajun_creole\n",
      "southern_us\n",
      "indian\n",
      "mexican\n",
      "southern_us\n",
      "korean\n",
      "brazilian\n",
      "mexican\n",
      "brazilian\n",
      "japanese\n",
      "mexican\n",
      "jamaican\n",
      "chinese\n",
      "korean\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "cajun_creole\n",
      "southern_us\n",
      "mexican\n",
      "mexican\n",
      "indian\n",
      "korean\n",
      "mexican\n",
      "french\n",
      "cajun_creole\n",
      "mexican\n",
      "indian\n",
      "jamaican\n",
      "korean\n",
      "mexican\n",
      "mexican\n",
      "southern_us\n",
      "mexican\n",
      "cajun_creole\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "indian\n",
      "chinese\n",
      "mexican\n",
      "greek\n",
      "mexican\n",
      "french\n",
      "french\n",
      "japanese\n",
      "mexican\n",
      "chinese\n",
      "chinese\n",
      "french\n",
      "cajun_creole\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "southern_us\n",
      "greek\n",
      "indian\n",
      "chinese\n",
      "mexican\n",
      "japanese\n",
      "japanese\n",
      "moroccan\n",
      "mexican\n",
      "jamaican\n",
      "japanese\n",
      "mexican\n",
      "southern_us\n",
      "chinese\n",
      "mexican\n",
      "japanese\n",
      "mexican\n",
      "mexican\n",
      "french\n",
      "mexican\n",
      "mexican\n",
      "moroccan\n",
      "southern_us\n",
      "southern_us\n",
      "mexican\n",
      "mexican\n",
      "korean\n",
      "southern_us\n",
      "brazilian\n",
      "mexican\n",
      "filipino\n",
      "southern_us\n",
      "japanese\n",
      "spanish\n",
      "mexican\n",
      "mexican\n",
      "vietnamese\n",
      "mexican\n",
      "chinese\n",
      "brazilian\n",
      "chinese\n",
      "chinese\n",
      "cajun_creole\n",
      "vietnamese\n",
      "british\n",
      "mexican\n",
      "korean\n",
      "mexican\n",
      "mexican\n",
      "chinese\n",
      "japanese\n",
      "japanese\n",
      "mexican\n",
      "mexican\n",
      "chinese\n",
      "spanish\n",
      "russian\n",
      "brazilian\n",
      "thai\n",
      "mexican\n",
      "mexican\n",
      "japanese\n",
      "mexican\n",
      "chinese\n",
      "brazilian\n",
      "jamaican\n",
      "thai\n",
      "spanish\n",
      "southern_us\n",
      "mexican\n",
      "mexican\n",
      "southern_us\n",
      "brazilian\n",
      "mexican\n",
      "indian\n",
      "greek\n",
      "mexican\n",
      "chinese\n",
      "indian\n",
      "chinese\n",
      "japanese\n",
      "indian\n",
      "greek\n",
      "mexican\n",
      "filipino\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "southern_us\n",
      "indian\n",
      "mexican\n",
      "mexican\n",
      "indian\n",
      "mexican\n",
      "japanese\n",
      "indian\n",
      "british\n",
      "indian\n",
      "brazilian\n",
      "mexican\n",
      "indian\n",
      "british\n",
      "chinese\n",
      "mexican\n",
      "mexican\n",
      "southern_us\n",
      "chinese\n",
      "chinese\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "japanese\n",
      "mexican\n",
      "mexican\n",
      "brazilian\n",
      "mexican\n",
      "southern_us\n",
      "southern_us\n",
      "filipino\n",
      "mexican\n",
      "moroccan\n",
      "indian\n",
      "irish\n",
      "french\n",
      "chinese\n",
      "jamaican\n",
      "thai\n",
      "korean\n",
      "indian\n",
      "jamaican\n",
      "mexican\n",
      "indian\n",
      "indian\n",
      "indian\n",
      "french\n",
      "mexican\n",
      "mexican\n",
      "chinese\n",
      "chinese\n",
      "southern_us\n",
      "southern_us\n",
      "mexican\n",
      "mexican\n",
      "russian\n",
      "mexican\n",
      "mexican\n",
      "indian\n",
      "brazilian\n",
      "mexican\n",
      "mexican\n",
      "brazilian\n",
      "filipino\n",
      "chinese\n",
      "british\n",
      "mexican\n",
      "brazilian\n",
      "brazilian\n",
      "japanese\n",
      "southern_us\n",
      "southern_us\n",
      "chinese\n",
      "brazilian\n",
      "southern_us\n",
      "southern_us\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "southern_us\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "spanish\n",
      "chinese\n",
      "korean\n",
      "southern_us\n",
      "mexican\n",
      "mexican\n",
      "greek\n",
      "filipino\n",
      "russian\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "japanese\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "thai\n",
      "british\n",
      "korean\n",
      "japanese\n",
      "mexican\n",
      "mexican\n",
      "chinese\n",
      "indian\n",
      "chinese\n",
      "chinese\n",
      "filipino\n",
      "mexican\n",
      "french\n",
      "southern_us\n",
      "mexican\n",
      "jamaican\n",
      "french\n",
      "indian\n",
      "french\n",
      "french\n",
      "brazilian\n",
      "mexican\n",
      "french\n",
      "cajun_creole\n",
      "southern_us\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "jamaican\n",
      "filipino\n",
      "southern_us\n",
      "brazilian\n",
      "korean\n",
      "british\n",
      "filipino\n",
      "filipino\n",
      "mexican\n",
      "mexican\n",
      "indian\n",
      "mexican\n",
      "mexican\n",
      "thai\n",
      "mexican\n",
      "french\n",
      "brazilian\n",
      "french\n",
      "chinese\n",
      "mexican\n",
      "french\n",
      "mexican\n",
      "mexican\n",
      "japanese\n",
      "french\n",
      "greek\n",
      "chinese\n",
      "chinese\n",
      "moroccan\n",
      "japanese\n",
      "japanese\n",
      "french\n",
      "brazilian\n",
      "mexican\n",
      "indian\n",
      "moroccan\n",
      "mexican\n",
      "southern_us\n",
      "french\n",
      "french\n",
      "chinese\n",
      "southern_us\n",
      "greek\n",
      "indian\n",
      "chinese\n",
      "mexican\n",
      "mexican\n",
      "korean\n",
      "chinese\n",
      "russian\n",
      "indian\n",
      "japanese\n",
      "moroccan\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "indian\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "indian\n",
      "mexican\n",
      "southern_us\n",
      "indian\n",
      "french\n",
      "indian\n",
      "mexican\n",
      "japanese\n",
      "french\n",
      "mexican\n",
      "southern_us\n",
      "mexican\n",
      "moroccan\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "irish\n",
      "greek\n",
      "mexican\n",
      "cajun_creole\n",
      "mexican\n",
      "mexican\n",
      "indian\n",
      "jamaican\n",
      "mexican\n",
      "brazilian\n",
      "greek\n",
      "russian\n",
      "mexican\n",
      "southern_us\n",
      "mexican\n",
      "mexican\n",
      "irish\n",
      "french\n",
      "mexican\n",
      "indian\n",
      "filipino\n",
      "mexican\n",
      "mexican\n",
      "indian\n",
      "southern_us\n",
      "chinese\n",
      "mexican\n",
      "mexican\n",
      "vietnamese\n",
      "mexican\n",
      "korean\n",
      "mexican\n",
      "french\n",
      "mexican\n",
      "mexican\n",
      "japanese\n",
      "chinese\n",
      "indian\n",
      "mexican\n",
      "mexican\n",
      "chinese\n",
      "mexican\n",
      "british\n",
      "japanese\n",
      "mexican\n",
      "mexican\n",
      "mexican\n",
      "japanese\n",
      "mexican\n",
      "mexican\n",
      "spanish\n",
      "indian\n",
      "mexican\n",
      "filipino\n",
      "spanish\n",
      "mexican\n",
      "brazilian\n",
      "mexican\n",
      "irish\n",
      "japanese\n",
      "indian\n",
      "southern_us\n"
     ]
    }
   ],
   "source": [
    "# are any of them not italian? My counts above claim that 1281 \n",
    "# weren't arbitrarily assigned\n",
    "\n",
    "print submission_data[n_test-2][1]\n",
    "\n",
    "for x in range(0, n_test):\n",
    "    if submission_data[x][1] != 'italian':\n",
    "        print submission_data[x][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, should be better than nothing..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV file creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "submission_1 = open('erh_submission_1_WC.csv', 'wb')\n",
    "submission_object = csv.writer(submission_1)\n",
    "submission_object.writerow(['id', 'cuisine'])\n",
    "\n",
    "for row in submission_data:\n",
    "    submission_object.writerow(submission_data[x])\n",
    "\n",
    "\n",
    "submission_1.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# good news, bad news\n",
    "\n",
    "The good news is there seems to have been something wrong with my algorithm (surprise, surprise!) because when I looked at the CSV file, every single entry was the same entry. which might be because the code above says \"submission_data[x]\" when row is it...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Redo the above code\n",
    "submission_1 = open('erh_submission_1_WC.csv', 'wb')\n",
    "submission_object = csv.writer(submission_1)\n",
    "submission_object.writerow(['id', 'cuisine'])\n",
    "\n",
    "for row in submission_data:\n",
    "    submission_object.writerow(row)\n",
    "\n",
    "\n",
    "submission_1.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I look at the csv file again and cross my fingers..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moderate success! \n",
    "Not every entry is Italian, so I am going to submit the sample where every one is italian, and then mine and see how much better I got. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Italian model - baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Score would have been 0.19268, place # 1367 (at the All Italian Benchmark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My baseline - submission # 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score would have been 0.26740, place # 1331. I guess that's my new baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
